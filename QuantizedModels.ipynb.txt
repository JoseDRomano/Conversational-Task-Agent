{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098c1cdc-33c5-4f35-aaca-8619c87f34b0",
   "metadata": {},
   "source": [
    "# Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f475a16-4d0f-4055-94b3-d0e03b420088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "#from peft import PeftModel\n",
    "from transformers import GenerationConfig,LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1ca501-9a15-49c6-a603-8af5ec2c1817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7fb852b52c10>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 41.3 GB\n",
      "  memory_reserved: \t 0.0 GB\n",
      "  memory_allocated: \t 0.0 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "def cuda_info():\n",
    "    print()\n",
    "    print(\"cuda.is_available: \\t\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print()\n",
    "        print(\"Cuda info:\")\n",
    "        print(\"  cuda driver version: \\t\", torch.version.cuda)\n",
    "        print(\"  cuda.device_count: \\t\", torch.cuda.device_count())\n",
    "        print()\n",
    "        print(\"Current_device: \\t\", torch.cuda.current_device())\n",
    "        print(\"  get_device_name: \\t\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        print(\"  device: \\t\\t\", torch.cuda.device(torch.cuda.current_device()))\n",
    "        print(\"  total_memory: \\t {0:2.1f} GB\".format(torch.cuda.get_device_properties(0).total_memory/1000/1000/1024))\n",
    "        print(\"  free_memory: \\t\\t {0:2.1f} GB\".format((torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1000/1000/1024))\n",
    "        print(\"  memory_reserved: \\t {0:2.1f} GB\".format(torch.cuda.memory_reserved(0)/1000/1000/1024))\n",
    "        print(\"  memory_allocated: \\t {0:2.1f} GB\".format(torch.cuda.memory_allocated(0)/1000/1000/1024))\n",
    "    \n",
    "    device = \"cuda:\" + str(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "    print()\n",
    "    print(\"device name: \\t\\t\", device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702c4bc-65c5-434e-94cc-fdf0b39a6870",
   "metadata": {},
   "source": [
    "# 4 bit model (GPUs with 6GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b70e4c7-8a58-4095-b30a-57c976347187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd8e7a6bba45e3aa8d79c559d53852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7fb85153e510>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 37.4 GB\n",
      "  memory_reserved: \t 4.2 GB\n",
      "  memory_allocated: \t 3.9 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/jmag/.conda/envs/wsdm2024/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/user/home/jmag/.conda/envs/wsdm2024/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/user/home/jmag/.conda/envs/wsdm2024/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/user/home/jmag/.conda/envs/wsdm2024/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\",\n",
    "        device_map='auto',\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        quantization_config=quantization_config\n",
    ")\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725abcd5-f658-4d34-887b-2ba6bd2a0db1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The House Budget Committee passed a spending bill. nobody knows what's in it.\n",
      "The House Budget Committee passed a spending bill. Nobody knows what's in it.\n",
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7f3d83a1f490>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 37.4 GB\n",
      "  memory_reserved: \t 4.4 GB\n",
      "  memory_allocated: \t 3.9 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The House Budget Committee passed a spending bill.'\n",
    "encoder_input_ids = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "greedy_outputs = model.generate(input_ids=encoder_input_ids, max_length=100)\n",
    "decoder_text = tokenizer.decode(greedy_outputs[0], skip_special_tokens=True)\n",
    "print(decoder_text)\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81913e0-6dce-4782-8fb6-7b6b9e4f3d99",
   "metadata": {},
   "source": [
    "## Model loading can be faster if you save the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ab4e81-c22a-4ece-8390-842d059521e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.temperature=None\n",
    "model.generation_config.top_p=None\n",
    "model.save_pretrained(\"nova/vicuna-7b-v1.5-4bit\", from_pt=True)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"nova/vicuna-7b-v1.5-4bit\",\n",
    "        device_map='auto',\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        temperature=None\n",
    ")\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f2b53-629b-43d8-bafc-65b43a3a7d8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8 bit model (GPUs with 8GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1801f192-2810-497a-b480-54acd38c86c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb394156de249158d4256c7b3d7d4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7fb8547eaa90>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 34.3 GB\n",
      "  memory_reserved: \t 7.0 GB\n",
      "  memory_allocated: \t 7.0 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n",
    "                                         llm_int8_threshold=200.0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\",\n",
    "        device_map='auto',\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        temperature=None,\n",
    "        quantization_config=quantization_config\n",
    ")\n",
    "cuda_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5266e696-a45b-4565-bddb-14e52f709ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The House Budget Committee passed a spending bill. 1. The bill includes funding for the Department of Defense, the Department of Labor, and the Department of Health and Human Services. 2. The bill also includes funding for other programs and agencies, such as the Environmental Protection Agency and the National Institutes of Health. 3. The bill is now being considered by the full House of Representatives. 4. If passed, the bill would go to the\n",
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7fb850e76690>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 34.3 GB\n",
      "  memory_reserved: \t 7.3 GB\n",
      "  memory_allocated: \t 7.0 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The House Budget Committee passed a spending bill. '\n",
    "encoder_input_ids = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "greedy_outputs = model.generate(input_ids=encoder_input_ids, max_length=100)\n",
    "decoder_text = tokenizer.decode(greedy_outputs[0], skip_special_tokens=True)\n",
    "print(decoder_text)\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2dfe85-12af-4d67-abcd-d48aca9e97fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 16 bit model (GPUs with 16GB or 24GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e170ff7-5b4d-4d26-bd07-58711bb6f9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa02c566c3c40d28988be693a0503e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7fb853e11150>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 28.1 GB\n",
      "  memory_reserved: \t 13.3 GB\n",
      "  memory_allocated: \t 13.3 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\", device_map='auto')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        temperature=None\n",
    ")\n",
    "\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1eb10b9-2ec8-49cc-bc7a-c39b30601fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The House Budget Committee passed a spending bill. 1. The bill includes $1.6 billion for border security, which is less than the $5 billion requested by the White House. 2. The bill also includes $1.3 billion for disaster relief, which is an increase from the previous year. 3. The bill includes $2.5 billion for military construction and veterans affairs, which is an increase from the previous year. 4. The bill includes\n",
      "\n",
      "cuda.is_available: \t True\n",
      "\n",
      "Cuda info:\n",
      "  cuda driver version: \t 12.1\n",
      "  cuda.device_count: \t 1\n",
      "\n",
      "Current_device: \t 0\n",
      "  get_device_name: \t NVIDIA A100-PCIE-40GB\n",
      "  device: \t\t <torch.cuda.device object at 0x7fb850644bd0>\n",
      "  total_memory: \t 41.3 GB\n",
      "  free_memory: \t\t 28.1 GB\n",
      "  memory_reserved: \t 13.5 GB\n",
      "  memory_allocated: \t 13.3 GB\n",
      "\n",
      "device name: \t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The House Budget Committee passed a spending bill. '\n",
    "encoder_input_ids = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "greedy_outputs = model.generate(input_ids=encoder_input_ids, max_length=100)\n",
    "decoder_text = tokenizer.decode(greedy_outputs[0], skip_special_tokens=True)\n",
    "print(decoder_text)\n",
    "cuda_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsdm2024",
   "language": "python",
   "name": "wsdm2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
