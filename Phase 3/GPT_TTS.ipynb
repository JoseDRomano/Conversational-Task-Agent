{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text summarization using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartConfig, BartModel\n",
    "\n",
    "# Initializing a BART facebook/bart-large style configuration\n",
    "configuration = BartConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the facebook/bart-large style configuration\n",
    "model = BartModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread it on sandwiches, toss it with pasta, or treat yourself a single happy spoonful, but definitely absolutely positively make pesto any chance you get.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Corn tortillas are made with just two ingredients: masa harina and water.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "import Phase1.recipe_parser as rp\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('../pickle_files/recipe_descs.pkl'):\n",
    "    descs = pickle.load(open('../pickle_files/recipe_descs.pkl', 'rb'))\n",
    "else:\n",
    "    descs = rp.get_recipe_descs()\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "print(descs[1])\n",
    "\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "inputs = tokenizer(descs[2], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 + TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from TTS.api import TTS\n",
    "\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "set_seed(42)\n",
    "recipe = \"Pesto Pasta\"\n",
    "text = \"The historical curiosity about \" + recipe + \" is\"\n",
    "Generated_Text = generator(text, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['The historical curiosity about Pesto Pasta is more than matched by the desire to know more about it, to figure out how and why we like it.', \"Today's blog contains links to two books that tell its story and describe what made it famous:\", 'The History of Pasta Pasta', 'And', 'The History of Baking Pasta', 'The History of Pesto Pasta was written by an engineer named Victor Dibbell.', 'A professor emeritus at Brown University']\n",
      " > Processing time: 70.91373085975647\n",
      " > Real-time factor: 2.2924159729094296\n"
     ]
    }
   ],
   "source": [
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=False)\n",
    "\n",
    "\n",
    "tts.tts_to_file(text=Generated_Text[0]['generated_text'],\n",
    "                file_path=\"../audio_files/output_\"+recipe+\".wav\",\n",
    "                speaker_wav=\"../audio_files/Sample en.wav\",\n",
    "                language=\"en\")\n",
    "\n",
    "count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
