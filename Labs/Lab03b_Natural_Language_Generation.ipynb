{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e1f6b4-d475-40ec-91b3-3ecca04a5cc2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Natural Language Generation\n",
    "\n",
    "For standard language generation:\n",
    " - https://huggingface.co/blog/how-to-generate\n",
    "  - https://huggingface.co/blog/introducing-csearch\n",
    "\n",
    "For constraint language generation:\n",
    " - https://huggingface.co/blog/constrained-beam-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512113c-b58b-4b4d-ad59-c296429a2dcd",
   "metadata": {},
   "source": [
    "## Auto-regressive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c87a2d-c5f1-425c-8ca7-1a6901f40486",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t False\n",
      "\n",
      "device name: \t\t cpu\n",
      "transformers: \t\t 4.38.2\n",
      "pytorch: \t\t 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import transformers\n",
    "from transformers import GenerationConfig, AutoTokenizer, AutoModel, utils, BartForConditionalGeneration \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "utils.logging.set_verbosity_error()  # Remove line to see warnings\n",
    "\n",
    "def cuda_info():\n",
    "    print()\n",
    "    print(\"cuda.is_available: \\t\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda.device_count: \\t\", torch.cuda.device_count())\n",
    "        print(\"cuda.current_device: \\t\", torch.cuda.current_device())\n",
    "        print(\"cuda.device: \\t\\t\", torch.cuda.device(torch.cuda.current_device()))\n",
    "        print()\n",
    "        print(\"cuda.get_device_name: \\t\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        print(\"total memory: \\t\\t\", torch.cuda.get_device_properties(0).total_memory)\n",
    "        print(\"reserved memory:\\t\", torch.cuda.memory_reserved(0))\n",
    "        print(\"allocated memory:\\t\", torch.cuda.memory_allocated(0))\n",
    "\n",
    "\n",
    "    device = \"cuda:\" + str(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "    print()\n",
    "    print(\"device name: \\t\\t\", device)\n",
    "    print(\"transformers: \\t\\t\", transformers.__version__)\n",
    "    print(\"pytorch: \\t\\t\", torch.__version__)\n",
    "    \n",
    "def decode_and_print(model, config, sentence):\n",
    "\n",
    "    encoded_input_ids_1 = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids = encoded_input_ids_1,\n",
    "            generation_config = generation_config,\n",
    "            return_dict_in_generate = True,\n",
    "            output_scores = True\n",
    "        )\n",
    "\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "        print(output)\n",
    "        \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5306af5-63ea-421c-aa60-5c0b65b649e8",
   "metadata": {},
   "source": [
    "# Decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23969f1f-8cdb-428f-a59b-ce2bb60cf74a",
   "metadata": {},
   "source": [
    "## DialogGPT\n",
    "\n",
    "https://huggingface.co/microsoft/DialoGPT-large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ae4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLANLLM MODEL\n",
    "moedl_plan = \"NOVA-vision-language/PlanLLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e62477-fbcc-4ad2-8849-fc40851c5b84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1654835924a14cf1851e2ae4971c351e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519ee7e84b1049d9a617d4ee9caf2ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c5db6c9fb14ddeab0e3bb6e07e9f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13423e61aa9942a68af4f73cc897b6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba21431bd994399ae5345a8e5bfbcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33630ea9c2d244249df1e80535807901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t False\n",
      "\n",
      "device name: \t\t cpu\n",
      "transformers: \t\t 4.38.2\n",
      "pytorch: \t\t 2.2.1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9001e4-8565-4142-b0c5-421e1dfa6a79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I do!\n",
      "DialoGPT: I'll be in the kitchen.\n",
      "DialoGPT: I know a few, but I don't know any good ones.\n",
      "DialoGPT: I will try to find one.\n",
      "DialoGPT: I will try to find one.\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ae51c-bf0e-492b-8a0d-845e6ea59b32",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e788c834-e350-433c-9ea6-bfaeda9f5705",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d37429292d430daf6d60eaedc82ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93729fc47afd4ebb97abaa6591b9a993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496038380f3b42fdb34900ab2afdac42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b07430d48f74e438dad461eafb44059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0434f53815f54df2bc0caf68d370ad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3329520527748a2b4f93109106dc8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t False\n",
      "\n",
      "device name: \t\t cpu\n",
      "transformers: \t\t 4.38.2\n",
      "pytorch: \t\t 2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model. Be sure to set output_attentions=True.\n",
    "# Load BART fine-tuned for summarization on CNN/Daily Mail dataset\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20062e5b-d18c-472b-9b0b-f1db0c0bce27",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decoding Strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751a268-03f6-43d9-a83f-fd35867684f6",
   "metadata": {},
   "source": [
    "## Decoding parameters and example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6d0c2b-8783-4b03-9d1b-e4bb4e9a7128",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_new_tokens\": 150,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"temperature\": 0.4,\n",
      "  \"top_k\": 10,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_config = model.generation_config\n",
    "\n",
    "generation_config.temperature = 0.4\n",
    "generation_config.top_p = 0.8\n",
    "generation_config.top_k = 10\n",
    "generation_config.num_beams = 4\n",
    "generation_config.max_new_tokens = 150\n",
    "\n",
    "print(generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9186b97d-1366-4ad5-a033-e30d5aa75117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joao/anaconda3/envs/nlp-cv-ir/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/joao/anaconda3/envs/nlp-cv-ir/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/Users/joao/anaconda3/envs/nlp-cv-ir/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was going too fast for the speed limit. the police wrote a ticket because they thought I was speeding. I'm not speeding, I'm driving too slowly, the police said. I was given a ticket for speeding.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create ids of encoded input vectors\n",
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast.'\n",
    "\n",
    "decode_and_print(model, generation_config, sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f4020-83f4-4dd8-942f-5c3e0f739646",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb5bfe5-7968-4356-9a8e-f57e468cd2dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.do_sample = False\n",
    "generation_config.num_beams = 1\n",
    "generation_config.max_new_tokens = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d72e195-676e-4b0d-ac19-58b3dcfdf19f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was speeding because I had a high speed limit. the police wrote a speedingticket because I drove too fast, not because I'm a speeding driver. the officer wrote me the ticket because he thought I was going too fast and I was in a hurry.\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast.'\n",
    "\n",
    "decode_and_print(model, generation_config, sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a3d2b-5909-4165-8c7e-1aad48b906c9",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da6886-b273-4695-bc86-c352c0553f3d",
   "metadata": {},
   "source": [
    "### Multinomial Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a2d8e-e79a-4945-bc74-0402b5b0b862",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fada6ee-2e3d-49fd-add6-bc5f75e019f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_sample\": true,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_new_tokens\": 150,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"top_k\": 10,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast.'\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.do_sample = True\n",
    "generation_config.num_beams = 1\n",
    "generation_config.temperature = 1\n",
    "\n",
    "print(generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5022f514-7cd4-4a64-bd8d-3ad420947ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Top k  10\n",
      "London police wrote me a speeding ticket because I was driving too fast. the London police wrote. me a ticket because of the speed I was going. the police wrote it because I drove too fast, not because I'm speeding. the officers wrote it as a result of my speed.\n",
      "\n",
      "## Top k  20\n",
      "London police wrote a speeding ticket to the author for driving too fast. The author's mother was also given a speeding citation for driving while pregnant. The driver was fined because she was driving too slowly. The ticket was issued because she had been driving too quickly. She was driving a car that was over 50 mph and had a speed limit of 30mph.\n",
      "\n",
      "## Top k  30\n",
      "London police wrote me a speeding ticket because I was driving too fast. the London police wrote us a speedingticket because I am driving tooFast. the police wrote an accident ticket because we are driving too Fast. we received a speeding summons because we were driving too slow.\n",
      "\n",
      "## Top k  40\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police got a speeding tickets for driving too faster. the police in London wrote me the ticket because  I wasdriving too fast. The London police were angry because they thought I was speeding.\n",
      "\n",
      "## Top k  50\n",
      "London police wrote me a speeding ticket because I was driving too fast. the London police wrote him a speedingticket because he was drivingtoo fast. It is not the first time London police have written a speeding Ticket to a driver. They have done it more than once.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "    \n",
    "    print(\"## Top k \", n*10)\n",
    "    generation_config.top_k = n*10\n",
    "    decode_and_print(model, generation_config, sentence)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc19464-b352-425f-aec6-058f2368ce14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Top-p sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6339d304-f3f5-4030-a8c0-8d41ccefbb30",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_sample\": true,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_new_tokens\": 150,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast.'\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.do_sample = True\n",
    "generation_config.num_beams = 1\n",
    "generation_config.temperature = 1\n",
    "\n",
    "print(generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bbfdc15-58aa-4a98-b9f4-b35ded0673b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Top p  0.15000000000000002\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was speeding because I had a high speed limit. the police wrote a speeding violation because I drove too fast and was driving to fast. I was not speeding. I had no idea I was going too fast, I was just driving too slow.\n",
      "\n",
      "## Top p  0.35000000000000003\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police write me a ticket becauseI was drivingTooFast. the police wrote the ticket because they thought I was speeding. the officer wrote the speeding ticket for driving too Fast. the policeman wrote the tickets because he thought I drove too fast and was driving TooFast.\n",
      "\n",
      "## Top p  0.55\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London policewrote me a ticket becauseI was drivingtoo fast. I was speeding because I wanted to get home before it was too late. I have been given a speeding warning and a £200 fine.\n",
      "\n",
      "## Top p  0.75\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police write me a speed ticket because i was driving Too Fast. the police wrote the ticket because of my driving tooFast. I was going to drive around 80mph but I was only doing 60. I had to slow down to a 45.\n",
      "\n",
      "## Top p  0.95\n",
      "London police wrote a speeding ticket on Tuesday. London police were writing up the car because it was driving too fast. The Briton was so worried he may injure himself, he pulled over. The speeding ticket actually said the car was moving too slow. The police officer was just trying to keep it under the legal limit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "    generation_config.top_p = 0.2*n-0.05\n",
    "    print(\"## Top p \", generation_config.top_p)\n",
    "    decode_and_print(model, generation_config, sentence)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d8d73-c519-4417-87ef-300f948224e9",
   "metadata": {},
   "source": [
    "### Contrastive Search\n",
    "https://huggingface.co/blog/introducing-csearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ccef8-2f34-471a-9f60-890c72c1bc29",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a6d9a9-e8e2-488d-b24c-3733b219c90c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "The London police wrote me a speeding ticket because I was driving too fast. The London police handed me a speed ticket becauseI was driving far too fast for the car's limit. the speeding ticket said I was speeding by driving too far. My passenger didn't know I was doing over 100mph.\n",
      "\n",
      "Output: \n",
      "London police wrote me a speeding ticket because I was driving too fast. the London police wrote the ticket because the car was driving as if it was under the influence of liquor. the mayor of London gave me a driving permit and a warning. after that I got a ticket for driving too very fast.\n",
      "\n",
      "Output: \n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police write me a freeway ticket because it was a speeders car. the car I was speed was going too fast because I was driving too far. I don't know where the ticket was written, but I don’t really care. I didn't think I was being excessive. I believe I was just going too soon. It's not a speeding fine, just a fine for going too quickly.\n",
      "\n",
      "Output: \n",
      "The London police wrote a speeding ticket because they said he was driving too fast. The London police were worried he was travelling too fast, they said. But the woman told her he had not done anything wrong. So she wrote the ticket because he was traveling too fast to stop.\n",
      "\n",
      "Output: \n",
      "LAPD wrote a speeding ticket for driving too fast, saying my car was driving too far. The police wrote the ticket because it was 'too soon for the conditions' The officer thought that Mr Hatter was driving his black Mazda 3 too fast. For more information, visit lpci.police.uk or click here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast.'\n",
    "\n",
    "encoded_input_ids_1 = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids = encoded_input_ids_1,\n",
    "        num_return_sequences=5, \n",
    "        generation_config = generation_config,\n",
    "        return_dict_in_generate = True,\n",
    "        output_scores = True\n",
    "    )\n",
    "\n",
    "for s in generation_output.sequences:\n",
    "    print(\"Output: \")\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6c3e4-7906-4f55-ae6b-11fa4b7461e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1217aadb-5d7f-4913-8796-8a564b0bb1a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast.'\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.do_sample = False\n",
    "generation_config.num_beams = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4271cf81-5c61-4914-931d-4f8de6ff441a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Beam size of  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joao/anaconda3/envs/nlp-cv-ir/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was speeding because I had a high speed limit. the police wrote a speedingticket because I drove too fast, not because I'm a speeding driver. the officer wrote me the ticket because he thought I was going too fast and I was in a hurry.\n",
      "\n",
      "## Beam size of  2\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was going too fast and wrote me the ticket. the police said that I was speeding and that I should slow down. I was not speeding. The police were writing me the speeding ticket for driving too quickly.\n",
      "\n",
      "## Beam size of  3\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was going too fast for the speed limit. the police said that I was doing too much speed. I was not speeding. The London police were writing me a ticket for driving too quickly.\n",
      "\n",
      "## Beam size of  4\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was going too fast for the speed limit. the police wrote a ticket because they thought I was speeding. I'm not speeding, I'm driving too slowly, the police said. I was given a ticket for speeding.\n",
      "\n",
      "## Beam size of  5\n",
      "The London police wrote me a speeding ticket because I was driving too fast. the London police said I was going too fast and wrote me the ticket. the police said it was because of the speed I was travelling at. I was not speeding, I was just driving too slowly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "\n",
    "    print(\"## Beam size of \", n)\n",
    "    generation_config.num_beams = n\n",
    "    decode_and_print(model, generation_config, sentence)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcae9c9-6f66-45f4-bb49-cac61ddd7b12",
   "metadata": {},
   "source": [
    "# Decoding with Constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69d88892-d8a0-4d62-88aa-77e5f61a1e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda.is_available: \t False\n",
      "\n",
      "device name: \t\t cpu\n",
      "transformers: \t\t 4.38.2\n",
      "pytorch: \t\t 2.2.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import transformers\n",
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "def cuda_info():\n",
    "    print()\n",
    "    print(\"cuda.is_available: \\t\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"cuda.device_count: \\t\", torch.cuda.device_count())\n",
    "        print(\"cuda.current_device: \\t\", torch.cuda.current_device())\n",
    "        print(\"cuda.device: \\t\\t\", torch.cuda.device(torch.cuda.current_device()))\n",
    "        print()\n",
    "        print(\"cuda.get_device_name: \\t\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        print(\"total memory: \\t\\t\", torch.cuda.get_device_properties(0).total_memory)\n",
    "        print(\"reserved memory:\\t\", torch.cuda.memory_reserved(0))\n",
    "        print(\"allocated memory:\\t\", torch.cuda.memory_allocated(0))\n",
    "\n",
    "\n",
    "    device = \"cuda:\" + str(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "    print()\n",
    "    print(\"device name: \\t\\t\", device)\n",
    "    print(\"transformers: \\t\\t\", transformers.__version__)\n",
    "    print(\"pytorch: \\t\\t\", torch.__version__)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b611331-7827-4639-a5a4-ea933cbc1a82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Repetitions and word lists\n",
    "### n-gram Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c17e212c-4890-4414-af02-5d212a6cad09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Yesterday the London police wrote me a speeding ticket because I was driving too fast.\n",
      "'I am\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Yesterday the London police wrote me a speeding ticket because I was driving too fast'\n",
    "\n",
    "encoded_input_ids_1 = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids = encoded_input_ids_1,\n",
    "        no_repeat_ngram_size=1,\n",
    "        return_dict_in_generate = True,\n",
    "        output_scores = True\n",
    "    )\n",
    "\n",
    "for s in generation_output.sequences:\n",
    "    print(\"Output: \")\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee34b2-7507-4651-8417-b4de73dd4243",
   "metadata": {},
   "source": [
    "### Force words and bad words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e84aee2-504c-434c-9218-d74bc603dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Force word ids:\n",
      "  DisjunctiveConstraint:  [[820, 734], [820, 530]]\n",
      "  PhrasalConstraint:  [47408, 783, 393, 4656]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The soldiers'\n",
    "input_ids = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "\n",
    "## Forced words\n",
    "force_disjunctive = [\"day two\", \"day one\"]\n",
    "force_phrasal = \"leave now or die\"\n",
    "\n",
    "force_words_ids = [ tokenizer(force_disjunctive, add_special_tokens=False).input_ids,\n",
    "                    tokenizer(force_phrasal, add_special_tokens=False).input_ids\n",
    "                  ]\n",
    "\n",
    "print(\"## Force word ids:\")\n",
    "for word_ids in force_words_ids:\n",
    "    if isinstance(word_ids[0], list):\n",
    "        print(\"  DisjunctiveConstraint: \", word_ids)\n",
    "    else:\n",
    "        print(\"  PhrasalConstraint: \", word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e294ceb-129f-40de-9096-23fa7eb93dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Bad word ids:\n",
      "PhrasalConstraint:  [1929, 296]\n",
      "PhrasalConstraint:  [1941]\n"
     ]
    }
   ],
   "source": [
    "## Bad words\n",
    "bad_words_set = [\"whom\", \"year\"]\n",
    "bad_words_ids = tokenizer(bad_words_set, add_special_tokens=False).input_ids\n",
    "\n",
    "print(\"## Bad word ids:\")\n",
    "for word_ids in bad_words_ids:\n",
    "    if isinstance(word_ids[0], list):\n",
    "        print(\"DisjunctiveConstraint: \", word_ids)\n",
    "    else:\n",
    "        print(\"PhrasalConstraint: \", word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb8db62e-f342-468d-b4c9-db59429b263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Output: \n",
      "The soldiers in the field were not the only ones who were injured.day twoleave now or die\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    bad_words_ids=bad_words_ids,\n",
    "    num_beams = 10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=6,\n",
    "    remove_invalid_values=True,\n",
    "    output_scores = True\n",
    ")\n",
    "\n",
    "for s in generation_output:\n",
    "    print(\"## Output: \")\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3f06a-aaa2-46b0-af95-fc7031f449f4",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef104a-de7d-433c-ac89-967ea5e8363a",
   "metadata": {},
   "source": [
    "### Phrasal Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bf0771b-150c-4264-8d7e-ed5394d6aef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The soldiers, who had been stationed at the base, had been ordered to leave the area.\n",
      "\n",
      "The soldiers, who were stationedat the base\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\").to(device)\n",
    "\n",
    "encoder_input_str = \"The soldiers\"\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "\n",
    "force_flexible_set = 'at the base'\n",
    "tk_list = tokenizer(force_flexible_set, add_special_tokens=False).input_ids\n",
    "\n",
    "constraints = [\n",
    "    PhrasalConstraint(tk_list)\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=5,\n",
    "    max_length = 30,\n",
    "    remove_invalid_values=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58b87d-0458-498a-a6fa-b7e9bc31e236",
   "metadata": {},
   "source": [
    "### Disjunctive Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6efa114c-a471-41e6-a7ff-0d36c49691a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25967], [3847]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint, DisjunctiveConstraint\n",
    "\n",
    "encoder_input_str = \"The soldiers\"\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "force_words_set1 = [\" stationed\", \"night\"]\n",
    "words_ids_set1 = tokenizer(force_words_set1, add_special_tokens=False).input_ids\n",
    "print(words_ids_set1)\n",
    "\n",
    "constraints = [\n",
    "    DisjunctiveConstraint(words_ids_set1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34642d97-d9a4-4a8e-8efc-6a443e5d0a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstationed'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(25967)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8da56ae7-5133-4c17-8c06-c5223b6304d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The soldiers, who had been stationed at the base, were taken to a nearby hospital, where they were treated for minor injuries and released.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    max_length = 30,\n",
    "    no_repeat_ngram_size=6,\n",
    "    remove_invalid_values=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f52bc5-0784-485c-a2f0-5715cecd59a3",
   "metadata": {},
   "source": [
    "### List of Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7df2be57-0be2-43ce-b6f0-d94e677c1fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[' stationed', 'in the field']\n",
      "{25967: {}, 259: {262: {2214: {}}}}\n",
      "\n",
      "[' hospital']\n",
      "{4436: {}}\n",
      "\n",
      " at the battle\n",
      "[379, 262, 3344]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint, DisjunctiveConstraint\n",
    "\n",
    "# The prompt\n",
    "encoder_input_str = \"The soldiers\"\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# First constraint\n",
    "force_words_set1 = [\" stationed\", \"in the field\"]\n",
    "words_ids_set1 = tokenizer(force_words_set1, add_special_tokens=False).input_ids\n",
    "constraint_1 = DisjunctiveConstraint(words_ids_set1)\n",
    "\n",
    "print()\n",
    "print(force_words_set1)\n",
    "print(constraint_1.trie.trie)\n",
    "\n",
    "# Second constraint\n",
    "force_words_set2 = [\" hospital\"]\n",
    "words_ids_set2 = tokenizer(force_words_set2, add_special_tokens=False).input_ids\n",
    "constraint_2 = DisjunctiveConstraint(words_ids_set2)\n",
    "\n",
    "print()\n",
    "print(force_words_set2)\n",
    "print(constraint_2.trie.trie)\n",
    "\n",
    "# Third constraint\n",
    "force_flexible_set = \" at the battle\"\n",
    "phrasal_constraints = tokenizer(force_flexible_set, add_special_tokens=False).input_ids\n",
    "constraint_3 = PhrasalConstraint(phrasal_constraints)\n",
    "\n",
    "print()\n",
    "print(force_flexible_set)\n",
    "print(constraint_3.token_ids)\n",
    "\n",
    "# The list of constraints\n",
    "constraints = [ constraint_1, constraint_2,constraint_3 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7ebd77b-cea5-40b9-b112-db8e79853389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The soldiers stationed at the base were not allowed to leave the base until the end of the war.\n",
      "\n",
      "\"We were told at the battle hospital\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    max_length = 30,\n",
    "    no_repeat_ngram_size=5,\n",
    "    remove_invalid_values=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162ab5c-b756-402f-b625-8d578db3065a",
   "metadata": {},
   "source": [
    "## Low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "979a7ff1-dc5e-4f8d-bcbc-1a0766d268b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The soldier, who was wearing a black T-shirt and jeans, said he had been in the country for two years.\\n\\n\"I was in the country for two years. I was in the country for two years,\" he said.black']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    ConstrainedBeamSearchScorer,\n",
    "    PhrasalConstraint, MaxLengthCriteria,\n",
    "    LogitsProcessorList, StoppingCriteriaList,\n",
    "    MinLengthLogitsProcessor\n",
    ")\n",
    "\n",
    "# lets run beam search using 3 beams\n",
    "num_beams = 3\n",
    "\n",
    "encoder_input_str = \"The soldier\"\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "input_ids = input_ids.repeat_interleave(num_beams, dim=0)\n",
    "\n",
    "constraint_str = [\"black\", \"country\"]\n",
    "constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n",
    "constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n",
    "\n",
    "# instantiate beam scorer\n",
    "beam_scorer = ConstrainedBeamSearchScorer(\n",
    "    batch_size=1, num_beams=num_beams, device=model.device, max_length = 50, constraints=constraints\n",
    ")\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = model.constrained_beam_search(\n",
    "    input_ids, beam_scorer, constraints=constraints, stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=50)]), logits_processor=logits_processor\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b77ad-f7a8-4b76-b1d5-cf20d1b031cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
