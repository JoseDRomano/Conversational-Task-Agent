{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# There are a lot of BERT based models available on HuggingFace,\n",
    "# and you have to pick one that is suitable for you.\n",
    "BERT_Model = \"bert-base-uncased\"\n",
    "\n",
    "# Initialise the BERT Transformer model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_Model)\n",
    "model = AutoModel.from_pretrained(BERT_Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the sentence embedding using BERT\n",
    "def sent_embedding(sent):\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    # This basically converts the sentence into a sequence of tokens\n",
    "    # Each token is either a complete word or a sub-word\n",
    "    tokens = tokenizer.encode_plus(sent, max_length=128, truncation=True,\n",
    "                                    padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    # Now feed the tokens into the model and get the embeddings as the output\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "    # Create an empty list to store two different kinds of embeddings\n",
    "    embedding_list = []\n",
    "\n",
    "    # last_hidden_state contains the output at the last hidden layer of all the sentence tokens\n",
    "    # pooler_output contains the embedding corresponding to only the [CLS] token, which in a way represents the whole sentence. \n",
    "    # This pooler_output is, however, different from the embeddings corresponding to the 1st token of last_hidden_state\n",
    "    # Although both represent the CLS token, the pooler_output is after some more processing, \n",
    "    # and more suitable for use in sentence classification tasks.\n",
    "\n",
    "    # This stores the embedding corresponding to the CLS token\n",
    "    embedding_list.append(outputs.last_hidden_state[0][0].detach().numpy().reshape(1,-1))\n",
    "\n",
    "    # This stores the embedding corresponding to the pooler_output\n",
    "    embedding_list.append(outputs.pooler_output.detach().numpy())\n",
    "\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2617, 0.6348, 0.2378, 0.2325, 0.3252, 0.1701, 0.4627, 0.2055, 0.4179,\n",
       "         0.1785, 0.1482, 0.1377, 0.2189, 0.2509, 0.2243, 0.2032, 0.0626, 0.1596,\n",
       "         0.2819, 0.1103, 0.0333, 0.2935, 0.3325, 0.3350, 0.2398, 0.3343, 0.2444,\n",
       "         0.1697, 0.3402, 0.2394, 0.2323, 0.1738, 0.1048, 0.2437, 0.1600, 0.2170,\n",
       "         0.2170, 0.1746, 0.2170, 0.2170, 0.4051, 0.1264, 0.1825, 0.2704, 0.1632,\n",
       "         0.3940, 0.3350, 0.2378, 0.4517, 0.1766, 0.2533, 0.1051, 0.2067, 0.2305,\n",
       "         0.1678, 0.2556, 0.1304, 0.4647, 0.1665, 0.2848, 0.2848, 0.2759, 0.3143,\n",
       "         0.2578, 0.3312, 0.2649, 0.2664, 0.3490, 0.1834, 0.1880, 0.2203, 0.4124,\n",
       "         0.3330, 0.3330, 0.3330, 0.3138, 0.2518, 0.2880, 0.2764, 0.2799, 0.2621,\n",
       "         0.2860, 0.0537, 0.2077, 0.1462, 0.2375, 0.2561, 0.2733, 0.2375, 0.2561,\n",
       "         0.1779, 0.2698, 0.1807, 0.2484, 0.3031, 0.2516, 0.3560, 0.3731, 0.2234,\n",
       "         0.1376, 0.4494, 0.5595, 0.6259, 0.5983, 0.3680, 0.4774, 0.2028, 0.1987,\n",
       "         0.2088, 0.2317, 0.1927, 0.2808, 0.3433, 0.4050, 0.1400, 0.3230, 0.3477,\n",
       "         0.3477, 0.2729, 0.3345, 0.2766, 0.2306, 0.2665, 0.3233, 0.2452, 0.1381,\n",
       "         0.1999, 0.3864, 0.1190, 0.1496, 0.2939, 0.2290, 0.2939, 0.2455, 0.2919,\n",
       "         0.2620, 0.2018, 0.1366, 0.0689, 0.2305, 0.3340, 0.2558, 0.1927, 0.2318,\n",
       "         0.1061, 0.3151, 0.1847, 0.1502, 0.1406, 0.2958, 0.1458, 0.0908, 0.0957,\n",
       "         0.1757, 0.1582, 0.1106, 0.1666, 0.1100, 0.0555, 0.1106, 0.0788, 0.0526,\n",
       "         0.0722, 0.3320, 0.1820, 0.2097, 0.1916, 0.2985, 0.2896, 0.3114, 0.1416,\n",
       "         0.1340, 0.1696, 0.0832, 0.2860, 0.3590, 0.4184, 0.3963, 0.3111, 0.2569,\n",
       "         0.3020, 0.3125, 0.2801, 0.2796, 0.1778, 0.1235, 0.1291, 0.1709, 0.4885,\n",
       "         0.3673, 0.3446, 0.3381, 0.3496, 0.3231, 0.3031, 0.2008, 0.1865, 0.2344,\n",
       "         0.3338, 0.5495, 0.0923, 0.2976, 0.2215, 0.1558, 0.1687, 0.2976, 0.2383,\n",
       "         0.2252, 0.2513, 0.1796, 0.2433, 0.2937, 0.3015, 0.2955, 0.2412, 0.3066,\n",
       "         0.2855, 0.3298, 0.1606, 0.2956, 0.5168, 0.2702, 0.2037, 0.3173, 0.1636,\n",
       "         0.1792, 0.3216, 0.0697, 0.2049, 0.2487, 0.5057, 0.3205, 0.3423, 0.1081,\n",
       "         0.1383, 0.2523, 0.4884, 0.3454, 0.2218, 0.3544, 0.3077, 0.2773, 0.2400,\n",
       "         0.1827, 0.2966, 0.3872, 0.2181, 0.3426, 0.4389, 0.3623, 0.3564, 0.1465,\n",
       "         0.4273, 0.3814, 0.4266, 0.3919, 0.2658, 0.3492, 0.3853, 0.4080, 0.1551,\n",
       "         0.1823, 0.1171, 0.1548, 0.2102, 0.2102, 0.1189, 0.2102, 0.2102, 0.2102,\n",
       "         0.1855, 0.2621, 0.1144, 0.1234, 0.2818, 0.2994, 0.1005, 0.2291, 0.2166,\n",
       "         0.1984, 0.1824, 0.2166, 0.2166, 0.2514, 0.2166, 0.2166, 0.2166, 0.2180,\n",
       "         0.1803, 0.1017, 0.1843, 0.5794, 0.3003, 0.2663, 0.2671, 0.1742, 0.3168,\n",
       "         0.1383, 0.4765, 0.3986, 0.4798, 0.3297, 0.0507, 0.2846, 0.2537, 0.0786,\n",
       "         0.1930, 0.3200, 0.3223, 0.2448, 0.1297, 0.3377, 0.2241, 0.3378, 0.3861,\n",
       "         0.2478, 0.2771, 0.2241, 0.2021, 0.2440, 0.1006, 0.1517, 0.2705, 0.2449,\n",
       "         0.1649, 0.3002, 0.2865, 0.1986, 0.2062, 0.2020, 0.2064, 0.3143, 0.3143,\n",
       "         0.3143, 0.3143, 0.3143, 0.3143, 0.3143, 0.3143, 0.3143, 0.3143, 0.2661,\n",
       "         0.2678, 0.2502, 0.2570, 0.1270, 0.2254, 0.1664, 0.2873, 0.3048, 0.2289,\n",
       "         0.2308, 0.1388, 0.1621, 0.1744, 0.2332, 0.1946, 0.2263, 0.2062, 0.3999,\n",
       "         0.1567, 0.1153, 0.2233, 0.1643, 0.2390, 0.2131, 0.2131, 0.5400, 0.5299,\n",
       "         0.2717, 0.1948, 0.2002, 0.1754, 0.2934, 0.2866, 0.3858, 0.3706, 0.2613,\n",
       "         0.2825, 0.2174, 0.1921, 0.2868, 0.3190, 0.3190, 0.3190, 0.3190, 0.3190,\n",
       "         0.3190, 0.3190, 0.3190, 0.3190, 0.1447, 0.2040, 0.1483, 0.1993, 0.2392,\n",
       "         0.2122, 0.2648, 0.2769, 0.0922, 0.1805, 0.2947, 0.2140, 0.1151, 0.1015,\n",
       "         0.2078, 0.2083, 0.3103, 0.3103, 0.2811, 0.2543, 0.4091, 0.2591, 0.4801,\n",
       "         0.1404, 0.2709, 0.3479, 0.3448, 0.2734, 0.2189, 0.1168, 0.2048, 0.0811,\n",
       "         0.1628, 0.1932, 0.1504, 0.2196, 0.2672, 0.3177, 0.2326, 0.2755, 0.2066,\n",
       "         0.2465, 0.1510, 0.3595, 0.2872, 0.2040, 0.2040, 0.2040, 0.2111, 0.2206,\n",
       "         0.1433, 0.1328, 0.2010, 0.1169, 0.2836, 0.2836, 0.1589, 0.2244, 0.2131,\n",
       "         0.3120, 0.1351, 0.3628, 0.1687, 0.1687, 0.1682, 0.2128, 0.1682, 0.1463,\n",
       "         0.2011, 0.2530, 0.1406, 0.1464, 0.2544, 0.4388, 0.3383, 0.3502, 0.1467,\n",
       "         0.1577, 0.2560, 0.3028, 0.2148, 0.1660, 0.2327, 0.3052, 0.1735, 0.2951,\n",
       "         0.4515, 0.4233, 0.3188, 0.1924, 0.3667, 0.0724, 0.1549, 0.2592, 0.2741,\n",
       "         0.2342, 0.2086, 0.2381, 0.1148, 0.1476, 0.3408, 0.1728, 0.2897, 0.1890,\n",
       "         0.1991, 0.1123, 0.1562, 0.2249, 0.1836, 0.3499, 0.2746, 0.2475, 0.1074,\n",
       "         0.1074, 0.1074, 0.1074, 0.1074, 0.1074, 0.1074, 0.1074, 0.1074, 0.1269,\n",
       "         0.4146, 0.2134, 0.2825, 0.1278, 0.6681, 0.1597, 0.1723, 0.1565, 0.3841,\n",
       "         0.1915, 0.2582, 0.2507, 0.3138, 0.2226, 0.2943, 0.1598, 0.2042, 0.2374,\n",
       "         0.0938, 0.2171, 0.2513, 0.1016, 0.2369, 0.2918, 0.2879, 0.2957, 0.2785,\n",
       "         0.2785, 0.2686, 0.2571, 0.2734, 0.2108, 0.5794, 0.1480, 0.2562, 0.2919,\n",
       "         0.2782, 0.1437, 0.0307, 0.2600, 0.3108, 0.3065, 0.2858, 0.3767, 0.3767,\n",
       "         0.3288, 0.3767, 0.3767, 0.3767, 0.1839, 0.1543, 0.2416, 0.1776, 0.1727,\n",
       "         0.2373, 0.1227, 0.2586, 0.1792, 0.1635, 0.2509, 0.3512, 0.3922, 0.5992,\n",
       "         0.3451, 0.3064, 0.5992, 0.3753, 0.3029, 0.5992, 0.3348, 0.5992, 0.3047,\n",
       "         0.1989, 0.2147, 0.2335, 0.2449, 0.2264, 0.1820, 0.2229, 0.2627, 0.1868,\n",
       "         0.2707, 0.1260, 0.2045, 0.1998, 0.3408, 0.2787, 0.1242, 0.2197, 0.2884,\n",
       "         0.1845, 0.2364, 0.2031, 0.1414, 0.1669, 0.1669, 0.1669, 0.1669, 0.1714,\n",
       "         0.1414, 0.1582, 0.1914, 0.1792, 0.1477, 0.2269, 0.4896, 0.1877, 0.1877,\n",
       "         0.1737, 0.1737, 0.1737, 0.1737, 0.1737, 0.1737, 0.1737, 0.3748, 0.2597,\n",
       "         0.3019, 0.2431, 0.1545, 0.2039, 0.3908, 0.3908, 0.6302, 0.4211, 0.2948,\n",
       "         0.2261, 0.4211, 0.3530, 0.3857, 0.4211, 0.3718, 0.2165, 0.1712, 0.3366,\n",
       "         0.3833, 0.3493, 0.3833, 0.3726, 0.3363, 0.2592, 0.3846, 0.3232, 0.3143,\n",
       "         0.3111, 0.1918, 0.2923, 0.2389, 0.2082, 0.2430, 0.2884, 0.1555, 0.3559,\n",
       "         0.1888, 0.2853, 0.3651, 0.3076, 0.2928, 0.2901, 0.2952, 0.2066, 0.1987,\n",
       "         0.2894, 0.2894, 0.2597, 0.2418, 0.0640, 0.2426, 0.2350, 0.1570, 0.2319,\n",
       "         0.2411, 0.1754, 0.3333, 0.3255, 0.2551, 0.3392, 0.2055, 0.2706, 0.4883,\n",
       "         0.3228, 0.1805, 0.2910, 0.2584, 0.2406, 0.1805, 0.2456, 0.0925, 0.1784,\n",
       "         0.0921, 0.1863, 0.3700, 0.4181, 0.2309, 0.1452, 0.2662, 0.2047, 0.1938,\n",
       "         0.0901, 0.2884, 0.1776, 0.2953, 0.2646, 0.2854, 0.2735, 0.2089, 0.3049,\n",
       "         0.2242, 0.2698, 0.2280, 0.5542, 0.2242, 0.2379, 0.2782, 0.2047, 0.1760,\n",
       "         0.3510, 0.2788, 0.1505, 0.1872, 0.2804, 0.2083, 0.1514, 0.1914, 0.2834,\n",
       "         0.3733, 0.2081, 0.3280, 0.2105, 0.3147, 0.2084, 0.2264, 0.1518, 0.1888,\n",
       "         0.2851, 0.2540, 0.1712, 0.1945, 0.2851, 0.3208, 0.3052, 0.2857, 0.2749,\n",
       "         0.3881, 0.2449, 0.0706, 0.1097, 0.1063, 0.1610, 0.3638, 0.2957, 0.3212,\n",
       "         0.4524, 0.4524, 0.4524, 0.4524, 0.4524, 0.1831, 0.2780, 0.0845, 0.2804,\n",
       "         0.0845, 0.2044, 0.2478, 0.2751, 0.1463, 0.2499, 0.3322, 0.2229, 0.3205,\n",
       "         0.3406, 0.2755, 0.2942, 0.2373, 0.3150, 0.1979, 0.1789, 0.1970, 0.3957,\n",
       "         0.2230, 0.2046, 0.1862, 0.2730, 0.2552, 0.2753, 0.1334, 0.2360, 0.1253,\n",
       "         0.3773, 0.3773, 0.3773, 0.3551, 0.3551, 0.2373, 0.1452, 0.0634, 0.0719,\n",
       "         0.1857, 0.2666, 0.2494, 0.2666, 0.3192, 0.2690, 0.2316, 0.2690, 0.3310,\n",
       "         0.2690, 0.2690, 0.2690, 0.2274, 0.2274, 0.2366, 0.2303, 0.1612, 0.1507,\n",
       "         0.1792, 0.1557, 0.2441, 0.2270, 0.2080, 0.1544, 0.1484, 0.1484, 0.1161,\n",
       "         0.0937, 0.2179, 0.1401, 0.1580, 0.5477, 0.2961, 0.2985, 0.2634, 0.2483,\n",
       "         0.3099, 0.2141, 0.2733, 0.2141, 0.2542, 0.2135, 0.2220, 0.2900, 0.1852,\n",
       "         0.2187, 0.2159, 0.2289, 0.2605, 0.2032, 0.1978, 0.1539, 0.2128, 0.2699,\n",
       "         0.2945, 0.1703, 0.3762, 0.1552, 0.1168, 0.2054, 0.3049, 0.3049, 0.3049,\n",
       "         0.3049, 0.3049, 0.3049, 0.3049, 0.3049, 0.3049, 0.1771, 0.1984, 0.2335,\n",
       "         0.1601, 0.2769, 0.2653, 0.2994, 0.2994, 0.2879, 0.3245, 0.3806, 0.2369,\n",
       "         0.3078, 0.2555, 0.1297, 0.2487, 0.2284, 0.2874, 0.0955, 0.1442, 0.1638,\n",
       "         0.2736, 0.1884, 0.1516, 0.2324, 0.1817, 0.0598, 0.1515, 0.4583, 0.2016,\n",
       "         0.2364, 0.1741, 0.1748, 0.1528, 0.1933, 0.2846, 0.1980, 0.1980, 0.1980,\n",
       "         0.1980, 0.1980, 0.2291, 0.1760, 0.1698, 0.1573, 0.2328, 0.1511, 0.1949,\n",
       "         0.1885, 0.1528, 0.3019, 0.2327, 0.0677, 0.2660, 0.1507, 0.3988, 0.2351,\n",
       "         0.5055, 0.2579, 0.2410, 0.2437, 0.1394, 0.2481, 0.1841, 0.2481, 0.1582,\n",
       "         0.2526, 0.2556, 0.5266, 0.2405, 0.1726, 0.3393, 0.1932, 0.2865, 0.2781,\n",
       "         0.4653, 0.1084, 0.2032, 0.2370, 0.3307, 0.3127, 0.2426, 0.2759, 0.2074,\n",
       "         0.1168, 0.4080, 0.2388, 0.0978, 0.1481, 0.2833, 0.0703, 0.3315, 0.2060,\n",
       "         0.2023, 0.2600, 0.2890, 0.2634, 0.2137, 0.3737, 0.3075, 0.2812, 0.3313,\n",
       "         0.2477, 0.1811, 0.2156, 0.0808, 0.1995, 0.2508, 0.3095, 0.2298, 0.1277,\n",
       "         0.1794, 0.1378, 0.2508, 0.1998]], device='mps:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import recipe_parser as rp\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('pickle_files/recipe_titles.pkl'):\n",
    "    titles = pickle.load(open('pickle_files/recipe_titles.pkl', 'rb'))\n",
    "else:\n",
    "    titles = rp.get_recipe_titles()\n",
    "\n",
    "# There are several different Sentence Transformer models available on Hugging Face\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "wanted_Recipe = \"Codfish\"\n",
    "\n",
    "\n",
    "# Convert the sentences into embeddings using the Sentence Transformer\n",
    "sent_embedding1 = model.encode(sent1,convert_to_tensor=True)\n",
    "sent_embedding2 = model.encode(titles,convert_to_tensor=True)\n",
    "\n",
    "# Find the similarity between the two embeddings\n",
    "util.pytorch_cos_sim(sent_embedding1, sent_embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesto Sauce from Scratch (Score: 0.6681)\n",
      "How to Make Pesto (Score: 0.6348)\n",
      "Vegetarian Pasta (Score: 0.6302)\n",
      "Italian Pasta Salad I (Score: 0.6259)\n",
      "Pasta Frittata (Score: 0.5992)\n"
     ]
    }
   ],
   "source": [
    "# return the top 5 most similar recipes\n",
    "top_k = 5\n",
    "top_results = util.pytorch_cos_sim(sent_embedding1, sent_embedding2)[0].topk(top_k)\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    print(titles[idx], \"(Score: {:.4f})\".format(score.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
